% !Mode:: "TeX:UTF-8"
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{参考文献}

% [1] Brandon Amos and J. Zico Kolter. OptNet: Differentiable optimization as a layer in neural networks. In ICML, 2017.

% [2] Shane Barratt. On the Differentiability of the Solution to Convex Optimization Problems. arXiv:1804.05098, 2018.

% [3] Luca Bertinetto, Joao F. Henriques, Philip H. S. Torr, and ˜ Andrea Vedaldi. Meta-learning with differentiable closedform solvers. In ICLR, 2019. 2, 4, 5, 6, 7, 8

% [4] Rich Caruana, Nikos Karampatziakis, and Ainur Yessenalina. An empirical evaluation of supervised learning in high dimensions. In ICML, 2008. 1

% [5] Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector machines. J. Mach. Learn. Res., 2:265–292, Mar. 2002. 1, 3

% [6] Justin Domke. Generic methods for optimization-based modeling. In AISTATS, 2012. 2

% [7] Asen L. Dontchev and R. Tyrrell Rockafellar. Implicit functions and solution mappings. Springer Monogr. Math., 2009. 4

% [8] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Modelagnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 1, 2, 3, 6, 7, 8

% [9] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V. Le. Dropblock: A regularization method for convolutional networks. In NeurIPS, 2018. 5

% [10] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In CVPR, 2018. 5, 6

% [11] Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison Guo. On differentiating parameterized argmin and argmax problems with application to bi-level optimization. arXiv preprint arXiv:1607.05447, 2016. 1, 2

% [12] Steven G. Krantz and Harold R. Parks. The implicit function theorem: history, theory, and applications. Springer Science & Business Media, 2012. 2, 4

% [13] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar- 100 (canadian institute for advanced research). 6

% [14] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, and Yi Yang. Transductive propagation network for few-shot learning. In ICLR, 2019. 6

% [15] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In ICML, 2015. 2

% [16] Tomasz Malisiewicz, Abhinav Gupta, and Alexei A. Efros. Ensemble of exemplar-svms for object detection and beyond. In ICCV, 2011. 1

% [17] Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriella Csurka. Distance-based image classification: Generalizing to new classes at near-zero cost. IEEE Trans. Pattern Anal. Mach. Intell., 35(11):2624–2637, Nov. 2013. 7

% [18] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In ICLR, 2018. 2, 5, 6

% [19] Tsendsuren Munkhdalai, Xingdi Yuan, Soroush Mehri, and Adam Trischler. Rapid adaptation with conditionally shifted neurons. In ICML, 2018. 2, 6

% [20] Boris N. Oreshkin, Pau Rodr´ıguez, and Alexandre Lacoste. Tadam: Task dependent adaptive metric for improved fewshot learning. In NeurIPS, 2018. 2, 5, 6, 7

% [21] Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L. Yuille. Few-shot image recognition by predicting parameters from activations. In CVPR, 2018. 5, 6, 8

% [22] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017. 1, 2, 3, 5, 6

% [23] Mengye Ren, Sachin Ravi, Eleni Triantafillou, Jake Snell, Kevin Swersky, Josh B. Tenenbaum, Hugo Larochelle, and Richard S. Zemel. Meta-learning for semi-supervised fewshot classification. In ICLR, 2018. 2, 5

% [24] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. Int. J. Comput. Vision, 115(3):211–252, Dec. 2015. 5

% [25] Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. In ICLR, 2019. 2, 5, 6, 8

% [26] Jurgen Schmidhuber. Evolutionary principles in selfreferential learning. on learning now to learn: The metameta-meta...-hook. Diploma thesis, Technische Universitat Munchen, Germany, 14 May 1987. 2

% [27] Uwe Schmidt and Stefan Roth. Shrinkage fields for effective image restoration. In CVPR, 2014. 2

% [28] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In NIPS, 2017. 1, 2, 3, 4, 5, 6, 7, 8

% [29] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H. S. Torr, and Timothy M. Hospedales. Learning to compare: Relation network for few-shot learning. In CVPR, 2018. 6, 7

% [30] Marshall F. Tappen, Ce Liu, Edward H. Adelson, and William T. Freeman. Learning gaussian conditional random fields for low-level vision. In CVPR, 2007. 2

% [31] Sebastian Thrun. Lifelong Learning Algorithms, pages 181– 209. Springer US, Boston, MA, 1998. 2

% [32] Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artificial Intelligence Review, 18(2):77–95, Jun 2002. 2

% [33] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In NIPS, 2016. 1, 2, 3, 5, 6, 7

% [34] Jason Weston and Chris Watkins. Support Vector Machines for Multiclass Pattern Recognition. In European Symposium On Artificial Neural Networks, 1999. 3

% [35] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In NIPS, 2014. 7

% [36] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016. 6
\nocite{*}
\bibliography{data/bibs}
\cleardoublepage