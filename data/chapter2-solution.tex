\chapter{模块2 解决方法}

\section{问题描述}

给定训练集 $D^{train} = \{(x_t,y_t)\}^{T}_{t=1}$ ，基学习器 $A$ 的目标是估计预测器 $y=f(x,\theta) $
的参数 $\theta$ 使得其在未见的测试集$D^{test} = \{(x_a, y_a)\}^{Q}_{t=1}$上泛化较好，
通常假定训练集和测试集采样于相同分布，并使用参数$\phi$为的嵌入模$f_{\phi}$型将样本域映射到特征空间。
对于基于优化的学习器，参数通过最小化训练数据上的经验损失以及正则项（鼓励模型简易）获得。

\begin{equation}
    \theta = A(D^{train}; \phi) = argmin_{\theta}L^{base}(D^{train}; \theta, \phi) + R(\theta)
\end{equation}

其中，$L^{base}$ 是损失函数，$R(\theta)$ 是正则项，在训练数据有限的情况下，正则项在模型的泛化方面扮演很重要的角色。

少量学习的元学习方法的目的是
的泛化误差最小。
任务分布中的泛化误差最小。具体来说，这
可以被认为是在一个任务的集合上学习：
T = {(Di train, Di test)}I i=1，通常被称为元训练集。元组（Ditrain, Ditest）描述了一个训练
和一个测试数据集，或一个任务。我们的目标是学习一个
嵌入模型φ，在给定基础学习者的情况下，使泛化（或测试）误差最小。
在给定基础学习者A的情况下，在不同的任务中使泛化（或测试）误差最小。
学习目标是：

\begin{equation}
    \min_{\phi} E_{T} [L^{meta}(D^{test}; \theta, \phi), where \theta = A(D^{train}, \phi)]
\end{equation}

图1说明了一个单一任务的训练和测试情况。
任务的训练和测试。一旦学会了嵌入模型fφ，它的泛化作用就会在一组保留的任务（通常被称为元测试集）上进行估计。
称为元测试集）S = {(Djtrain, Djtest)}J j=1 计算出来的。
为：

\begin{equation}
    
\end{equation}

根据之前的工作[22，8]，我们把估计方程2和3中的期望值的阶段分别称为元训练和
元测试。在元训练期间，我们保留了一个
额外保留的元验证集来选择元学习器的超参数并挑选最佳嵌入
模型。

\section{创新思想}
本文将可微的二次规划求解器和不同的线性分类器综合起来。利用以上两个特性，本文实现了在
计算成本略有增加的情况下提供了比最临近分类器更大的收益。

本文的方法使用线性分类器，因为它能够规划为凸学习问题。

本文的工作主要与用后向传播进行过程优化的技术相关。

\section{具体方法}

任务（episode）集：少样本学习benmarks如miniImageNet[22]以K-way，N-shot分类任务估计模型，这里K表示类数，
N表示每个类的训练样本数。少量样本学习技巧由小值的N选择来评价，典型地。实践上，这些数据集并不显式包含，
但元学习的每个任务可以通过在元训练阶段中即兴（on-the-fly）构造，称（task）之为一个episode。例如在工作[33,22]，一个任务（或episode）
以如下方式被采样到:

总体类集为, 对于每个episode，首先类(包含来自的K类)被有放回抽样得到，然后训练集(每个类包含N个图像)被采样；最后测试集（每个类包含Q个图像）被采样；

注意强调需要无放回抽样，如，以优化泛化误差。以同样的方式从和各自即兴构造元验证meta-validation集和元测试meta-test集。
为了度量嵌入模型对未见类的泛化，需被互斥选择；

由于后续需要计算任务期望，基学习器必须是高效的，并且为了估计嵌入模型的参数，
关于的梯度计算也必须是高效的，这就鼓励选择简单的基学习器如最近类均值nearest-class-mean[28]，
其参数是很容易计算的且目标函数是可微的。

文中考虑基于多类线性分类器如SVMs[5,34]，LR,岭回归，
因它们的目标函数都是凸性的。例如k类线性SVM可以写作，
k类SVM的Crammer-Singer [ on the algorithmic implementation of multiclass kernel-based vector machines ] 
规划为

其中，C是正则化参数，是Kronecker delta 函数；


为了使得系统端到端可训练，要求SVM求解器关于输入应该是可微的，
例如应该能够计算\left\{ \frac{\partial{\theta}}{\partial{f_{\phi}(x_{n})}}  \right\} _{n=1}^{N\times K}；
SVM的目标是凸的(具有唯一最优值),这允许运用关于最优条件（KKT）的隐函数理论[12,7,2]获得必要的梯度。


为了完备性的缘故，下面推导凸优化问题的定理形式，考虑下面的凸优化问题：

其中向量是问题的优化变量，向量是优化问题的输入参数，即。

通过求解下面拉格朗日函数的鞍点来优化目标:

换句话说可以通过获得目标函数最优值，


给定函数，用表示其雅可比矩阵;

定理1（自on the differentiability of the solution to convex optimization problems） 假定，若所有导数存在，则

这个结果由对KKT条件应用隐函数定理所得，因此，一旦计算得到最优解，便可以获得的梯度关于输入数据的闭式表达式，这就消除了通过完全的优化路径进行后向传播的必要，因为这个解决方案由于其唯一性，它并不依赖路径或初始化，这也节省了内存，这正是凸优化问题比一般优化问题的优势所在。

时间复杂度：在前向传递过程（方程4）中使用此方法要求进行QP求解，时间复杂度为，其中d是优化变量的数量，时间主要消耗在分解KKT矩阵（原始对偶内部点方法所用）。后向传递过程要求以定理1求解方程8，其复杂度是，（假定在前向传递过程中已经进行了分解）。当嵌入的维度较大时，前向和后向传递代价都比较高。

对偶规划：方程4的目标对偶规划允许处理对嵌入维度上的低依赖性，故可以重写为如下式，
令
\begin{equation}
    w_{k}(\alpha^{k}) = \sum_{n}{\alpha_{n}^{k} f_{\phi}(x_{n})} \quad \forall k.
\end{equation}

可以在对偶空间优化  
\begin{equation}
    max_{\alpha ^{k}} \big[ -\frac{1}{2} \sum_{k}\parallel\omega_{k}(\alpha^{k})\parallel_{2}^{2} + \sum_{n}\alpha_{n}^{y_{n}}\big]
    \\ subject \ to \quad \alpha_{n}^{y_{n}} \leq C, \ \alpha_{n}^{k} \leq 0 \quad \forall k \neq y_{n} , 
    \\ \qquad \sum_{k}{\alpha_{n}^{k}=0 \quad \forall n.} \tag{10}
\end{equation}

这产生了一个在对偶变量上的二次规划（QP），
注意这里优化变量的数量是训练样本数量乘以类数，
对于少样本学习而言这通常要比特征维度的数量要小。
使用[OptNet:differentiable optimization as a layer in neural networks]
（实现了一个可微的基于GPU的QP求解器）解方程10的对偶二次规划。
实践上，QP求解所耗时间同时使用ResNet-12结构计算特征所耗时间是近似的，
因此每次迭代的总体速度同基于简单基学习器的方法（如Prototypical Networks中使用的最近类原型均值[28]）并没有大的不同。

与此本同时，[Meta-learning with differentiable closed-form solvers]采用岭回归作为基学习器，
也有一个闭式解。尽管岭回归或许不是最适合分类问题的，其工作表明通过最小化关于one-hot labels的平方误差来训练模型在实践上工作得很好。
对于岭回归，最终的优化也是一个QP，因此也能够在本文的框架中实现：

其中定义同方程9。

线性SVM和岭回归之间的比较表明线性SVM规划具有稍微优势。

元学习和嵌入模型：对于少量样本学习的元学习方法，目标在于最小化（采样于）同任务分布下的不同任务的泛化误差，
具体讲就是在任务集合上学习，即在元训练集上学习，其中描述了一个训练和测试集（即一个任务），
学习目标是给定一个基学习器，学习一个嵌入模型能够最小化这些任务上的泛化误差：



一旦学习了嵌入模型，其泛化能力由held-out任务（也称meta-test集）所估计：

在元训练阶段中，保持额外的held-out meta-validation 集来选择元学习器的超参和挑选最好的嵌入模型；

为了度量模型的性能，从相同任务采样出测试数据来计算负对数似然。因此，可以重新表达方程2的元学习目标为：

其中，是一个可学习的尺度参数。

先前的少样本学习工作[20,3,10]暗示，在使用最近类均值和岭回归基学习器的方法中，
通过一个可学习的尺度参数调节预测分数提供了更好的性能。经验地发现插入对使用SVM作为基学习器的元学习也是有益的。
尽管选择其他的测试损失函数，如合页损失，也是可以的，但对数似然方式在实验中工作的最好。
