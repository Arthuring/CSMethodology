\chapter{模块3（实验与分析）}

\section{创新思想和学术贡献}

\section{实验方法}
我们首先描述了在实验中使用的网络架构和优化细节（第4.1节）。
然后，我们展示了在标准的几种少样本分类基准上的结果，包括ImageNet的派生数据集（第4.2节）和CIFAR（第4.3节），
随后使用相同的嵌入网络和训练设置对各种基础学习器对准确性和速度的影响进行了详细分析（第4.4-4.6节）。
\subsection{实验细节}
元学习设置。我们在实验中使用了一个ResNet-12网络，遵循[20，18]。
令Rk表示由三个{3×3卷积和k个滤波器，批归一化，Leaky ReLU(0.1)}组成的残差块；令MP表示2×2最大池化。我们使用了DropBlock正则化[9]，
一种结构化Dropout形式。令DB(k，b)表示具有保持率为k和块大小为b的DropBlock层。
用于ImageNet衍生数据集的网络架构为：R64-MP-DB(0.9,1)-R160-MP-DB(0.9,1)-R320-MP-DB(0.9,5)-R640-MP-DB(0.9,5)，
而用于CIFAR衍生数据集的网络架构为：R64-MP-DB(0.9,1)-R160-MP-DB(0.9,1)-R320-MP-DB(0.9,2)-R640-MP-DB(0.9,2)。
在最后的残差块后，我们不应用全局平均池化。

作为优化器，我们使用带有0.9的Nesterov动量和0.0005的权重衰减的SGD。
每个小批次包含8个样本集。该模型进行了60个epochs的元训练，每个epoch包含1000个样本集。
学习率最初设置为0.1，然后在第20、40和50个epoch时分别更改为0.006、0.0012和0.00024，这是遵循[10]的实践。

在元训练期间，我们采用了水平翻转、随机裁剪和颜色（亮度、对比度和饱和度）扰动数据增强，如[10, 21]中所述。
对于使用ResNet-12的miniImageNet实验，我们使用eps = 0.1的标签平滑。与[28]使用元训练的高路径分类不同，
我们在两个阶段都使用5路分类，这是遵循最近的工作[10，20]。每个类别在元训练期间包含6个测试（查询）样本，
在元测试期间包含15个测试样本。我们的元训练模型是基于元验证集上5路5-shot测试准确性选择的。

对于原型网络，我们将元训练shot设置为与元测试shot相匹配，这是遵循通常的做法[28, 10]。
对于SVM和岭回归，我们观察到保持元训练shot高于元测试shot可以获得更好的测试准确性，如图2所示。因此，在元训练期间，
我们针对ResNet-12的miniImageNet将训练shot设置为15；对于使用4层CNN的miniImageNet（在表3中）将训练shot设置为5；
对于tieredImageNet，将训练shot设置为10；对于CIFAR-FS，将训练shot设置为5；对于FC100，将训练shot设置为15。


基学习器的设置。对于线性分类器的训练，我们使用二次规划求解器OptNet [1]。
SVM的正则化参数C设置为0.1。岭回归的正则化参数λ设置为50.0。对于最近类均值（原型网络），
我们使用针对特征维度进行归一化的平方欧几里得距离。

提前停止。虽然我们可以运行优化器直到收敛，但在实践中，我们发现在固定的迭代次数（仅三次）
内运行QP求解器实际上效果很好。提前停止起到了额外的正则化作用，甚至可以带来稍微更好的性能。

\section{实验结果}
